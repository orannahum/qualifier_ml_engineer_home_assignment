{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from preprocessing import preprocess_text\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load my model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'oranne55/qualifier-model4-finetune-pretrained-transformer-for-long-inputs'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function predict_long_text_with_preprocess processes a list of texts by first preprocessing and tokenizing each text. It splits long texts into chunks (512 tokens with 100 tokens overlap), classifies each chunk using a model, and checks if any chunk is classified as \"jailbreak.\" The final classification for each text is determined by whether any chunk contains \"jailbreak\". The overlapping happens to maintain the connection between the different parts of the text. It then returns a list of predictions (\"jailbreak\" or \"benign\") for each text. The model is run on the available device (MPS or CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding Window Overlapping](sliding_window_overlaping.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict_long_text_with_preprocess(texts, model, tokenizer):\n",
    "    predictions = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Preprocess and tokenize the text into chunks\n",
    "        text = preprocess_text(text)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "\n",
    "        # Define chunk size and overlap\n",
    "        chunk_size = 512\n",
    "        overlap_size = 100\n",
    "\n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(input_ids), chunk_size - overlap_size):\n",
    "            chunk = input_ids[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Flag to track if any chunk is classified as \"jailbreak\"\n",
    "        contains_jailbreak = False\n",
    "        for chunk in chunks:\n",
    "            chunk = chunk.unsqueeze(0).to(device)  # Move chunk to the correct device (MPS or CPU)\n",
    "\n",
    "            # Predict on the chunk\n",
    "            with torch.no_grad():\n",
    "                outputs = model(chunk)\n",
    "                logits = outputs.logits\n",
    "                prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "                # Check if this chunk is classified as \"jailbreak\"\n",
    "                if model.config.id2label[prediction] == \"jailbreak\":\n",
    "                    contains_jailbreak = True\n",
    "                    break  # Stop further checks if \"jailbreak\" is detected\n",
    "\n",
    "        # Final decision based on whether any chunk was classified as \"jailbreak\"\n",
    "        final_prediction = \"jailbreak\" if contains_jailbreak else \"benign\"\n",
    "        predictions.append(final_prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benign', 'benign']\n"
     ]
    }
   ],
   "source": [
    "example_texts = [\n",
    "    \"This is a example text.\",\n",
    "    \"This is a example2 text.\",]\n",
    "\n",
    "predictions = predict_long_text_with_preprocess(example_texts, model, tokenizer)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualifier-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

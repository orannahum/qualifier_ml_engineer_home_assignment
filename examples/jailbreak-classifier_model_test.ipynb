{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"jackhhao/jailbreak-classifier\")\n",
    "dataset = load_dataset(\"jackhhao/jailbreak-classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'type'],\n",
       "        num_rows: 1044\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'type'],\n",
       "        num_rows: 262\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all metrics from the evaluate library\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "precision_metric = evaluate.load(\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "F1 Score: 0.9747292418772563\n",
      "Accuracy: 0.9732824427480916\n",
      "Recall: 0.9712230215827338\n",
      "Precision: 0.9782608695652174\n"
     ]
    }
   ],
   "source": [
    "# Define a function to evaluate predictions on the test set\n",
    "def evaluate_model(model_pipeline, dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        # Get model prediction with truncation\n",
    "        prediction = model_pipeline(sample[\"prompt\"], truncation=True)[0]\n",
    "        \n",
    "        # Map text labels to numerical labels\n",
    "        if prediction[\"label\"].lower() == \"benign\":\n",
    "            predicted_label = 0\n",
    "        elif prediction[\"label\"].lower() == \"jailbreak\":\n",
    "            predicted_label = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected label format: {prediction['label']}\")\n",
    "\n",
    "        predictions.append(predicted_label)\n",
    "        references.append(1 if sample[\"type\"].lower() == \"jailbreak\" else 0)  # Adjust label mapping accordingly\n",
    "\n",
    "    # Compute each metric\n",
    "    f1_score = f1_metric.compute(predictions=predictions, references=references, average=\"binary\")\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=references, average=\"binary\")\n",
    "    precision = precision_metric.compute(predictions=predictions, references=references, average=\"binary\")\n",
    "\n",
    "    return {\n",
    "        \"F1 Score\": f1_score[\"f1\"],\n",
    "        \"Accuracy\": accuracy[\"accuracy\"],\n",
    "        \"Recall\": recall[\"recall\"],\n",
    "        \"Precision\": precision[\"precision\"]\n",
    "    }\n",
    "\n",
    "# Evaluate on the test set\n",
    "metrics = evaluate_model(pipe, test_dataset)\n",
    "print(\"Evaluation Results:\")\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qualifier-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
